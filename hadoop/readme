
# Make scripts executable

chmod +x "/home/eolus/Desktop/Dauphine/bigdata/tfidf/hadoop/deploy_tf.sh
chmod +x "/home/eolus/Desktop/Dauphine/bigdata/tfidf/hadoop/deploy_df.sh
chmod +x "/home/eolus/Desktop/Dauphine/bigdata/tfidf/hadoop/deploy_tfidf.sh


# Delete any existing documents


# Load documents from weburls



# Before we run the actual MapReduce job, we must first copy the files from our local file system to Hadoopâ€™s HDFS.

fpath_data = "/home/eolus/Desktop/Dauphine/bigdata/tfidf/data/16713-8.txt"
hdfs dfs -ls '/home/eolus/Desktop/Dauphine/bigdata/tfidf/data/16713-8.txt'
